# wasi-nn Image Classification Example

This example demonstrates running machine learning inference using wasi-nn with OpenVINO backend on Propeller.

## Overview

- **Runtime**: wasmtime 39.0.1 with wasi-nn support
- **Backend**: OpenVINO 2025.4.0
- **Model**: MobileNet v2 (image classification)
- **Architecture**: x86_64/amd64 only (OpenVINO requirement)

## Prerequisites

1. **Docker** with Docker Compose
2. **Rust** with `wasm32-wasip1` target
3. **Go** for building propeller-cli (optional, pre-built available)

## Quick Start

### Step 1: Clone and Setup Propeller

```bash
git clone https://github.com/absmach/propeller.git
cd propeller
```

### Step 2: Download ML Model Files

The example uses MobileNet for image classification. Create the fixture directory and download model files:

```bash
mkdir -p fixture
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/mobilenet.bin -O fixture/model.bin
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/mobilenet.xml -O fixture/model.xml
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/tensor-1x224x224x3-f32.bgr -O fixture/tensor.bgr
```

### Step 3: Build the WASM Example

```bash
# Clone wasmtime repository
git clone https://github.com/bytecodealliance/wasmtime.git
cd wasmtime/crates/wasi-nn/examples/classification-example

# Install wasm32-wasip1 target if not already installed
rustup target add wasm32-wasip1

# Build the example
cargo build --target wasm32-wasip1 --release

# Verify the build
ls -la target/wasm32-wasip1/release/wasi-nn-example.wasm
```

### Step 4: Setup Directories

Create the required directories in the propeller root:

```bash
cd propeller

# Create models directory
mkdir -p models

# Create fixture directory and download model files
mkdir -p fixture
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/mobilenet.bin -O fixture/model.bin
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/mobilenet.xml -O fixture/model.xml
wget https://download.01.org/openvinotoolkit/fixtures/mobilenet/tensor-1x224x224x3-f32.bgr -O fixture/tensor.bgr
```

Your directory structure should look like:
```
propeller/
├── config.toml      # Generated by propeller-cli provision
├── models/          # ML models directory
├── fixture/         # wasi-nn model files
│   ├── model.bin
│   ├── model.xml
│   └── tensor.bgr
├── docker/
│   ├── compose.yaml
│   └── .env
└── ...
```

### Step 5: Start Propeller Services

```bash
cd propeller

# Start all services
docker compose -f docker/compose.yaml --env-file docker/.env up -d

# Build proplet with wasi-nn/OpenVINO support (first time only)
docker compose -f docker/compose.yaml --env-file docker/.env build proplet

# Restart proplet with new image
docker compose -f docker/compose.yaml --env-file docker/.env up -d proplet
```

### Step 6: Provision (First Time Only)

```bash
propeller-cli provision
```

This creates the necessary SuperMQ resources and generates `config.toml`.

### Step 7: Build the CLI (Optional)

If you need the `--cli-args` flag support:

```bash
go build -o build/cli ./cmd/cli
```

### Step 8: Create and Run the wasi-nn Task

```bash
# Create task with wasi-nn CLI arguments
./build/cli tasks create wasi-nn-inference \
    --cli-args="-S,nn,--dir=/home/proplet/fixture::fixture"
```

You'll get a response like:
```json
{
  "cli_args": ["-S", "nn", "--dir=/home/proplet/fixture::fixture"],
  "id": "e0ad2be5-af77-476f-a2b8-23fc41fa8d00",
  "name": "wasi-nn-inference",
  ...
}
```

### Step 9: Upload the WASM File

```bash
# Set the task ID from the previous response
TASK_ID="<your-task-id-here>"

# Upload the WASM file
curl -X PUT "http://localhost:7070/tasks/${TASK_ID}/upload" \
    -F "file=@/path/to/wasmtime/crates/wasi-nn/examples/classification-example/target/wasm32-wasip1/release/wasi-nn-example.wasm"
```

### Step 10: Start the Task

```bash
./build/cli tasks start $TASK_ID
```

### Step 11: Check Results

```bash
# View proplet logs
docker compose -f docker/compose.yaml logs proplet --tail 50

# Or check task status
curl -s http://localhost:7070/tasks/$TASK_ID | jq
```

## Expected Output

When the task runs successfully, the proplet logs will show:

```
Read graph XML, first 50 characters: <?xml version="1.0" ?>
<net name="mobilenet_v2_1.0
Read graph weights, size in bytes: 13956476
Loaded graph into wasi-nn with ID: 0
Created wasi-nn execution context with ID: 0
Read input tensor, size in bytes: 602112
Executed graph inference
Found results, sorted top 5: [InferenceResult(885, 0.3958259), InferenceResult(904, 0.36464667), InferenceResult(84, 0.010480282), InferenceResult(911, 0.008229051), InferenceResult(741, 0.007244824)]
```

The numbers are ImageNet class IDs:
- `885` = "quill" (feather pen)
- `904` = "window shade"

## Complete Example Script

Here's a complete script that runs the full workflow:

```bash
#!/bin/bash
set -e

PROPELLER_DIR="/path/to/propeller"
WASMTIME_DIR="/path/to/wasmtime"
FIXTURE_DIR="/path/to/fixture"

cd $PROPELLER_DIR

# Create task
echo "Creating wasi-nn task..."
RESPONSE=$(./build/cli tasks create wasi-nn-test \
    --cli-args="-S,nn,--dir=/home/proplet/fixture::fixture" 2>&1)

TASK_ID=$(echo "$RESPONSE" | jq -r '.id')
echo "Task ID: $TASK_ID"

# Upload WASM
echo "Uploading WASM file..."
curl -s -X PUT "http://localhost:7070/tasks/${TASK_ID}/upload" \
    -F "file=@${WASMTIME_DIR}/crates/wasi-nn/examples/classification-example/target/wasm32-wasip1/release/wasi-nn-example.wasm"

# Start task
echo "Starting task..."
./build/cli tasks start $TASK_ID

# Wait and check logs
sleep 3
echo "=== Proplet Logs ==="
docker compose -f docker/compose.yaml logs proplet --tail 20
```

## CLI Arguments Explained

When creating a task, the `--cli-args` flag passes arguments to wasmtime:

| Argument | Description |
|----------|-------------|
| `-S nn` | Enables wasi-nn support in wasmtime |
| `--dir=/home/proplet/fixture::fixture` | Maps host directory to guest `fixture` directory |

The format is: `--dir=<host-path>::<guest-path>`

## Task Manifest (Alternative)

Instead of using CLI flags, you can also create tasks via curl with a JSON body:

```bash
curl -X POST http://localhost:7070/tasks \
    -H "Content-Type: application/json" \
    -d '{
        "name": "wasi-nn-classification",
        "cli_args": ["-S", "nn", "--dir=/home/proplet/fixture::fixture"],
        "daemon": false
    }'
```

## Troubleshooting

### "OpenVINO version is too old"
The Dockerfile uses OpenVINO 2025.4.0. If you see this error, rebuild the proplet image:
```bash
docker compose -f docker/compose.yaml --env-file docker/.env build --no-cache proplet
```

### "libtbb.so.12: cannot open shared object file"
The `libtbb12` package is missing. This should be installed automatically by the Dockerfile.

### "Unknown error" at wasi_nn::load()
1. Check fixture directory is mounted: `docker exec propeller-proplet ls /home/proplet/fixture`
2. Verify model files exist: `model.xml`, `model.bin`, `tensor.bgr`
3. Check OpenVINO environment: `docker exec propeller-proplet env | grep -i openvino`

### Task not starting / MQTT errors
1. Check manager and proplet are connected: `docker compose logs manager proplet --tail 20`
2. Verify provisioning: `cat config.toml`
3. Restart services: `docker compose restart manager proplet`

### Apple Silicon (M1/M2/M3) Users
OpenVINO requires x86_64. The proplet runs under emulation via Docker's Rosetta/QEMU. This is handled automatically by `platform: linux/amd64` in compose.yaml.

## Architecture

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  propeller-cli  │────▶│     Manager     │────▶│     Proplet     │
│  (tasks create) │     │  (HTTP :7070)   │     │  (MQTT client)  │
└─────────────────┘     └─────────────────┘     └────────┬────────┘
                                                         │
                                                         ▼
                                                ┌─────────────────┐
                                                │    wasmtime     │
                                                │   -S nn         │
                                                │   --dir=fixture │
                                                └────────┬────────┘
                                                         │
                                                         ▼
                                                ┌─────────────────┐
                                                │    OpenVINO     │
                                                │   2025.4.0      │
                                                └─────────────────┘
```

## References

- [wasi-nn Specification](https://github.com/WebAssembly/wasi-nn)
- [wasmtime wasi-nn Documentation](https://docs.wasmtime.dev/wasi-nn.html)
- [OpenVINO Installation](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- [MobileNet Model](https://download.01.org/openvinotoolkit/fixtures/mobilenet/)
